# ç¬¬4ç« : ä¸­ç´šç·¨ - StatsDataReaderã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

## ğŸ“š ã“ã®ç« ã§å­¦ã¶ã“ã¨

- StatsDataReaderã‚¯ãƒ©ã‚¹ã®è©³ç´°ãªä½¿ã„æ–¹
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå–å¾—ï¼ˆè‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- æ§˜ã€…ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—æˆ¦ç•¥

**å¯¾è±¡ãƒ¬ãƒ™ãƒ«**: ä¸­ç´šè€…
**æ‰€è¦æ™‚é–“**: ç´„60åˆ†
**å‰æçŸ¥è­˜**: ç¬¬1ã€œ3ç« ã®å†…å®¹

---

## 4.1 StatsDataReaderã¨ã¯

### 4.1.1 æ¦‚è¦

`StatsDataReader`ã¯ã€e-Statã‹ã‚‰å®Ÿéš›ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ç¬¬3ç« ã§å­¦ã‚“ã ãƒ¡ã‚¿æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€åŠ¹ç‡çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã™ã€‚

### 4.1.2 ä¸»ãªæ©Ÿèƒ½

- âœ… **è‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³**: 10ä¸‡ä»¶ã‚’è¶…ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚‚è‡ªå‹•ã§åˆ†å‰²å–å¾—
- âœ… **æŸ”è»Ÿãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: åœ°åŸŸã€æ™‚é–“ã€ã‚«ãƒ†ã‚´ãƒªãªã©ã§çµã‚Šè¾¼ã¿
- âœ… **åŠ¹ç‡çš„ãªå–å¾—**: å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’å–å¾—
- âœ… **ãƒ‡ãƒ¼ã‚¿å¤‰æ›**: è‡ªå‹•çš„ã«pandas DataFrameã«å¤‰æ›

---

## 4.2 åŸºæœ¬çš„ãªä½¿ã„æ–¹

### 4.2.1 ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿å–å¾—

```python
"""
StatsDataReaderã®åŸºæœ¬çš„ãªä½¿ã„æ–¹
"""
import os
from dotenv import load_dotenv
from jpy_datareader.estat import StatsDataReader
import pandas as pd

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# StatsDataReaderã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
reader = StatsDataReader()

# çµ±è¨ˆè¡¨IDã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
stats_id = "0003410379"  # ä¾‹: å®¶è¨ˆèª¿æŸ»

# ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df = reader.read(
    statsDataId=stats_id,
    limit=100  # æœ€åˆã®100ä»¶ã‚’å–å¾—
)

# çµæœã‚’ç¢ºèª
print(f"å–å¾—ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã€‘")
print(df.head())

# ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’è¡¨ç¤º
print("\nã€ã‚«ãƒ©ãƒ ä¸€è¦§ã€‘")
print(df.columns.tolist())
```

### 4.2.2 åˆ©ç”¨å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
"""
StatsDataReaderã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
"""
from jpy_datareader.estat import StatsDataReader

reader = StatsDataReader()

df = reader.read(
    # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    statsDataId="0003410379",    # çµ±è¨ˆè¡¨ID

    # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    limit=1000,                   # å–å¾—ä»¶æ•°ï¼ˆæœ€å¤§100,000ï¼‰
    startPosition=1,              # å–å¾—é–‹å§‹ä½ç½®

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    cdArea=None,                  # åœ°åŸŸã‚³ãƒ¼ãƒ‰
    cdCat01=None,                 # ã‚«ãƒ†ã‚´ãƒª1ã®ã‚³ãƒ¼ãƒ‰
    cdCat02=None,                 # ã‚«ãƒ†ã‚´ãƒª2ã®ã‚³ãƒ¼ãƒ‰
    cdCat03=None,                 # ã‚«ãƒ†ã‚´ãƒª3ã®ã‚³ãƒ¼ãƒ‰
    cdTime=None,                  # æ™‚é–“ã‚³ãƒ¼ãƒ‰

    # ãã®ä»–
    lang="J"                      # è¨€èª (J: æ—¥æœ¬èª, E: è‹±èª)
)
```

---

## 4.3 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½

### 4.3.1 åœ°åŸŸã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
"""
åœ°åŸŸã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""
from jpy_datareader.estat import MetaInfoReader, StatsDataReader
from dotenv import load_dotenv

load_dotenv()

def get_data_by_area(stats_id, area_name, limit=1000):
    """
    åœ°åŸŸåã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    area_name : str
        åœ°åŸŸåï¼ˆä¾‹: 'æ±äº¬éƒ½'ï¼‰
    limit : int
        å–å¾—ä»¶æ•°

    Returns:
    --------
    pandas.DataFrame
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰åœ°åŸŸã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    meta_reader = MetaInfoReader()
    df_meta = meta_reader.read(statsDataId=stats_id)

    # åœ°åŸŸåˆ†é¡ã‚’æŠ½å‡º
    df_area = df_meta[df_meta['@class'] == 'area']

    # åœ°åŸŸåã§æ¤œç´¢
    df_found = df_area[df_area['@name'].str.contains(area_name, na=False)]

    if len(df_found) == 0:
        print(f"âŒ '{area_name}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    # æœ€åˆã®è©²å½“åœ°åŸŸã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    area_code = df_found.iloc[0]['@code']
    area_full_name = df_found.iloc[0]['@name']

    print(f"âœ… åœ°åŸŸ: {area_full_name} [{area_code}]")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    data_reader = StatsDataReader()
    df = data_reader.read(
        statsDataId=stats_id,
        cdArea=area_code,  # åœ°åŸŸã‚³ãƒ¼ãƒ‰ã‚’æŒ‡å®š
        limit=limit
    )

    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")

    return df


# ===== ä½¿ç”¨ä¾‹ =====
print("ã€åœ°åŸŸåˆ¥ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

# æ±äº¬éƒ½ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df_tokyo = get_data_by_area(stats_id, "æ±äº¬éƒ½", limit=500)

if df_tokyo is not None:
    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã€‘")
    print(df_tokyo.head())

    # åŸºæœ¬çµ±è¨ˆé‡
    numeric_cols = df_tokyo.select_dtypes(include=['float64', 'int64']).columns
    if len(numeric_cols) > 0:
        print("\nã€åŸºæœ¬çµ±è¨ˆé‡ã€‘")
        print(df_tokyo[numeric_cols].describe())
```

### 4.3.2 æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
"""
æ™‚é–“ã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""
from jpy_datareader.estat import MetaInfoReader, StatsDataReader
from dotenv import load_dotenv

load_dotenv()

def get_data_by_time(stats_id, time_keyword, limit=1000):
    """
    æ™‚é–“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    time_keyword : str
        æ™‚é–“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: '2024'ï¼‰
    limit : int
        å–å¾—ä»¶æ•°

    Returns:
    --------
    pandas.DataFrame
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    # ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰æ™‚é–“ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
    meta_reader = MetaInfoReader()
    df_meta = meta_reader.read(statsDataId=stats_id)

    # æ™‚é–“åˆ†é¡ã‚’æŠ½å‡º
    df_time = df_meta[df_meta['@class'] == 'time']

    # æ™‚é–“ã§æ¤œç´¢
    df_found = df_time[df_time['@name'].str.contains(time_keyword, na=False)]

    if len(df_found) == 0:
        print(f"âŒ '{time_keyword}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    print(f"âœ… {len(df_found)}ä»¶ã®æ™‚é–“ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    # æœ€åˆã®è©²å½“æ™‚é–“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
    time_code = df_found.iloc[0]['@code']
    time_name = df_found.iloc[0]['@name']

    print(f"   ä½¿ç”¨ã™ã‚‹æ™‚é–“: {time_name} [{time_code}]")

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    data_reader = StatsDataReader()
    df = data_reader.read(
        statsDataId=stats_id,
        cdTime=time_code,  # æ™‚é–“ã‚³ãƒ¼ãƒ‰ã‚’æŒ‡å®š
        limit=limit
    )

    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")

    return df


# ä½¿ç”¨ä¾‹
print("ã€æ™‚é–“åˆ¥ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

# 2024å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df_2024 = get_data_by_time(stats_id, "2024", limit=500)

if df_2024 is not None:
    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã€‘")
    print(df_2024.head(10))
```

### 4.3.3 è¤‡æ•°æ¡ä»¶ã§ã®çµã‚Šè¾¼ã¿

```python
"""
è¤‡æ•°ã®æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""
from jpy_datareader.estat import MetaInfoReader, StatsDataReader
from dotenv import load_dotenv

load_dotenv()

def get_filtered_data(
    stats_id,
    area_keyword=None,
    time_keyword=None,
    cat01_keyword=None,
    limit=1000
):
    """
    è¤‡æ•°æ¡ä»¶ã§ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚“ã§å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    area_keyword : str, optional
        åœ°åŸŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    time_keyword : str, optional
        æ™‚é–“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    cat01_keyword : str, optional
        ã‚«ãƒ†ã‚´ãƒª1ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    limit : int
        å–å¾—ä»¶æ•°

    Returns:
    --------
    pandas.DataFrame
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    # ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
    meta_reader = MetaInfoReader()
    df_meta = meta_reader.read(statsDataId=stats_id)

    # ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    params = {
        'statsDataId': stats_id,
        'limit': limit
    }

    # åœ°åŸŸã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    if area_keyword:
        df_area = df_meta[df_meta['@class'] == 'area']
        df_found = df_area[df_area['@name'].str.contains(area_keyword, na=False)]

        if len(df_found) > 0:
            params['cdArea'] = df_found.iloc[0]['@code']
            print(f"âœ… åœ°åŸŸ: {df_found.iloc[0]['@name']} [{params['cdArea']}]")

    # æ™‚é–“ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    if time_keyword:
        df_time = df_meta[df_meta['@class'] == 'time']
        df_found = df_time[df_time['@name'].str.contains(time_keyword, na=False)]

        if len(df_found) > 0:
            params['cdTime'] = df_found.iloc[0]['@code']
            print(f"âœ… æ™‚é–“: {df_found.iloc[0]['@name']} [{params['cdTime']}]")

    # ã‚«ãƒ†ã‚´ãƒª1ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    if cat01_keyword:
        df_cat = df_meta[df_meta['@class'] == 'cat01']
        df_found = df_cat[df_cat['@name'].str.contains(cat01_keyword, na=False)]

        if len(df_found) > 0:
            params['cdCat01'] = df_found.iloc[0]['@code']
            print(f"âœ… ã‚«ãƒ†ã‚´ãƒª: {df_found.iloc[0]['@name']} [{params['cdCat01']}]")

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    data_reader = StatsDataReader()
    df = data_reader.read(**params)

    print(f"\nâœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")

    return df


# ===== ä½¿ç”¨ä¾‹ =====
print("ã€è¤‡æ•°æ¡ä»¶ã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

# å¤§é˜ªåºœã®2024å¹´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df = get_filtered_data(
    stats_id,
    area_keyword="å¤§é˜ªåºœ",
    time_keyword="2024",
    limit=500
)

if df is not None and len(df) > 0:
    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã€‘")
    print(df.head())

    # ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜
    output_file = "osaka_2024_data.csv"
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
```

---

## 4.4 å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆè‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

### 4.4.1 è‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®ä»•çµ„ã¿

```python
"""
å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–å¾—
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv
import time

load_dotenv()

def get_large_dataset(stats_id, max_records=None):
    """
    å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    max_records : int, optional
        æœ€å¤§å–å¾—ä»¶æ•°ï¼ˆNoneã®å ´åˆã¯ã™ã¹ã¦å–å¾—ï¼‰

    Returns:
    --------
    pandas.DataFrame
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    reader = StatsDataReader()

    # limitã‚’æŒ‡å®šã—ãªã„ã€ã¾ãŸã¯Noneã«ã™ã‚‹ã¨è‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
    start_time = time.time()

    print(f"ã€å¤§é‡ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ã€‘")
    print("=" * 60)
    print(f"çµ±è¨ˆè¡¨ID: {stats_id}")

    if max_records:
        df = reader.read(statsDataId=stats_id, limit=max_records)
    else:
        # limitã‚’æŒ‡å®šã—ãªã„ã¨å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        df = reader.read(statsDataId=stats_id)

    elapsed_time = time.time() - start_time

    print(f"\nâœ… å–å¾—å®Œäº†")
    print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
    print(f"   ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}åˆ—")
    print(f"   æ‰€è¦æ™‚é–“: {elapsed_time:.2f}ç§’")

    return df


# ä½¿ç”¨ä¾‹
stats_id = "0003410379"

# 10,000ä»¶å–å¾—
df = get_large_dataset(stats_id, max_records=10000)

if df is not None:
    print("\nã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
    print(df.info())
```

### 4.4.2 æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³

```python
"""
æ‰‹å‹•ã§ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ¶å¾¡
"""
from jpy_datareader.estat import StatsDataReader
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

def manual_pagination(stats_id, page_size=10000, max_pages=5):
    """
    æ‰‹å‹•ã§ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ¶å¾¡ã—ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    page_size : int
        1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ä»¶æ•°
    max_pages : int
        æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

    Returns:
    --------
    pandas.DataFrame
        çµåˆã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    reader = StatsDataReader()
    all_data = []

    print(f"ã€æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã€‘")
    print("=" * 60)

    for page in range(max_pages):
        start_pos = page * page_size + 1

        print(f"\nãƒšãƒ¼ã‚¸ {page + 1}/{max_pages} ã‚’å–å¾—ä¸­...")
        print(f"  é–‹å§‹ä½ç½®: {start_pos}")

        try:
            df_page = reader.read(
                statsDataId=stats_id,
                startPosition=start_pos,
                limit=page_size
            )

            if len(df_page) == 0:
                print(f"  ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆå–å¾—çµ‚äº†ï¼‰")
                break

            all_data.append(df_page)
            print(f"  âœ… {len(df_page)}ä»¶å–å¾—")

            # å–å¾—ä»¶æ•°ãŒpage_sizeã‚ˆã‚Šå°‘ãªã‘ã‚Œã°çµ‚äº†
            if len(df_page) < page_size:
                print(f"  æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”")
                break

        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            break

    # ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    if all_data:
        df_combined = pd.concat(all_data, ignore_index=True)
        print(f"\nâœ… ç·å–å¾—ä»¶æ•°: {len(df_combined):,}ä»¶")
        return df_combined
    else:
        print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None


# ä½¿ç”¨ä¾‹
stats_id = "0003410379"

df = manual_pagination(stats_id, page_size=5000, max_pages=3)

if df is not None:
    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã¨æœ«å°¾ã€‘")
    print("\nå…ˆé ­5è¡Œ:")
    print(df.head())
    print("\næœ«å°¾5è¡Œ:")
    print(df.tail())
```

---

## 4.5 ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

### 4.5.1 åŸºæœ¬çš„ãªå‰å‡¦ç†

```python
"""
å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çš„ãªå‰å‡¦ç†
"""
from jpy_datareader.estat import StatsDataReader
import pandas as pd
import numpy as np
from dotenv import load_dotenv

load_dotenv()

def preprocess_data(df):
    """
    ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†

    Parameters:
    -----------
    df : pandas.DataFrame
        å…ƒã®ãƒ‡ãƒ¼ã‚¿

    Returns:
    --------
    pandas.DataFrame
        å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
    """
    print("ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã€‘")
    print("=" * 60)

    # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
    df_clean = df.copy()

    # ===== ã‚¹ãƒ†ãƒƒãƒ—1: æ¬ æå€¤ã®ç¢ºèª =====
    print("\nã‚¹ãƒ†ãƒƒãƒ—1: æ¬ æå€¤ã®ç¢ºèª")
    print("-" * 60)

    missing_counts = df_clean.isnull().sum()
    missing_cols = missing_counts[missing_counts > 0]

    if len(missing_cols) > 0:
        print("æ¬ æå€¤ãŒã‚ã‚‹ã‚«ãƒ©ãƒ :")
        for col, count in missing_cols.items():
            percentage = (count / len(df_clean)) * 100
            print(f"  {col}: {count}ä»¶ ({percentage:.1f}%)")
    else:
        print("âœ… æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“")

    # ===== ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèªã¨å¤‰æ› =====
    print("\nã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª")
    print("-" * 60)

    # æ•°å€¤ã£ã½ã„ã‚«ãƒ©ãƒ ã‚’è‡ªå‹•æ¤œå‡º
    for col in df_clean.columns:
        # æ–‡å­—åˆ—å‹ã®ã‚«ãƒ©ãƒ ã§æ•°å€¤ã«å¤‰æ›ã§ããã†ãªã‚‚ã®
        if df_clean[col].dtype == 'object':
            # æ•°å€¤ã«å¤‰æ›ã‚’è©¦ã¿ã‚‹
            try:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
                converted = df_clean[col].notna().sum()
                if converted > 0:
                    print(f"  {col}: æ•°å€¤å‹ã«å¤‰æ› ({converted}ä»¶)")
            except:
                pass

    # ===== ã‚¹ãƒ†ãƒƒãƒ—3: é‡è¤‡ã®ç¢ºèª =====
    print("\nã‚¹ãƒ†ãƒƒãƒ—3: é‡è¤‡ã®ç¢ºèª")
    print("-" * 60)

    duplicates = df_clean.duplicated().sum()
    if duplicates > 0:
        print(f"âš ï¸ é‡è¤‡è¡Œ: {duplicates}ä»¶")
        print("  é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
        # ã“ã“ã§ã¯è‡ªå‹•çš„ã«å‰Šé™¤
        df_clean = df_clean.drop_duplicates()
        print(f"  âœ… é‡è¤‡ã‚’å‰Šé™¤ã—ã¾ã—ãŸï¼ˆæ®‹ã‚Š: {len(df_clean)}ä»¶ï¼‰")
    else:
        print("âœ… é‡è¤‡ã¯ã‚ã‚Šã¾ã›ã‚“")

    # ===== ã‚¹ãƒ†ãƒƒãƒ—4: åŸºæœ¬çµ±è¨ˆé‡ =====
    print("\nã‚¹ãƒ†ãƒƒãƒ—4: åŸºæœ¬çµ±è¨ˆé‡")
    print("-" * 60)

    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"æ•°å€¤ã‚«ãƒ©ãƒ æ•°: {len(numeric_cols)}")
        print("\nåŸºæœ¬çµ±è¨ˆé‡:")
        print(df_clean[numeric_cols].describe())

    print(f"\nâœ… å‰å‡¦ç†å®Œäº†ï¼ˆ{len(df_clean)}ä»¶ï¼‰")

    return df_clean


# ===== ä½¿ç”¨ä¾‹ =====
reader = StatsDataReader()
stats_id = "0003410379"

# ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df_raw = reader.read(statsDataId=stats_id, limit=1000)

print(f"ã€å…ƒãƒ‡ãƒ¼ã‚¿ã€‘{len(df_raw)}ä»¶\n")

# å‰å‡¦ç†
df_processed = preprocess_data(df_raw)

# çµæœã‚’ä¿å­˜
df_processed.to_csv("processed_data.csv", index=False, encoding='utf-8-sig')
print("\nâœ… å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
```

---

## ğŸ“ ç·´ç¿’å•é¡Œ

### å•é¡Œ1: åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—

**èª²é¡Œ**: StatsDataReaderã‚’ä½¿ã£ã¦çµ±è¨ˆè¡¨ID "0003410379" ã‹ã‚‰1000ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ï¼ˆè¡Œæ•°Ã—åˆ—æ•°ï¼‰ã¨ã‚«ãƒ©ãƒ åã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- `StatsDataReader()`ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
- `read(statsDataId=..., limit=1000)`
- `df.shape`ã§å½¢çŠ¶ã‚’ç¢ºèª
- `df.columns.tolist()`ã§ã‚«ãƒ©ãƒ åã‚’å–å¾—

</details>

---

### å•é¡Œ2: åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

**èª²é¡Œ**: ã€Œç¦å²¡çœŒã€ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’500ä»¶å–å¾—ã—ã€æ•°å€¤ã‚«ãƒ©ãƒ ã®åŸºæœ¬çµ±è¨ˆé‡ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- MetaInfoReaderã§åœ°åŸŸã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
- StatsDataReaderã§`cdArea`ã‚’æŒ‡å®š
- `select_dtypes(include=['float64', 'int64'])`ã§æ•°å€¤ã‚«ãƒ©ãƒ ã‚’æŠ½å‡º
- `describe()`ã§çµ±è¨ˆé‡ã‚’è¡¨ç¤º

</details>

---

### å•é¡Œ3: è¤‡æ•°æ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

**èª²é¡Œ**: ã€ŒåŒ—æµ·é“ã€ã®ã€Œ2023å¹´ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- åœ°åŸŸã¨æ™‚é–“ã®ä¸¡æ–¹ã§ãƒ•ã‚£ãƒ«ã‚¿
- `cdArea`ã¨`cdTime`ã‚’ä¸¡æ–¹æŒ‡å®š
- `to_csv()`ã§ä¿å­˜

</details>

---

### å•é¡Œ4: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³

**èª²é¡Œ**: æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ã€10,000ä»¶ãšã¤3ãƒšãƒ¼ã‚¸åˆ†ï¼ˆåˆè¨ˆ30,000ä»¶ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€çµåˆã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- `for`ãƒ«ãƒ¼ãƒ—ã§3å›å–å¾—
- `startPosition`ã‚’1, 10001, 20001ã¨å¤‰æ›´
- `pd.concat()`ã§çµåˆ

</details>

---

### å•é¡Œ5: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

**èª²é¡Œ**: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ãŸå¾Œã€ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
1. æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
2. é‡è¤‡è¡Œã‚’å‰Šé™¤
3. æ•°å€¤ã‚«ãƒ©ãƒ ã®å¹³å‡å€¤ã‚’è¨ˆç®—
4. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜

<details>
<summary>ãƒ’ãƒ³ãƒˆ</summary>

- `dropna()`ã§æ¬ æå€¤å‰Šé™¤
- `drop_duplicates()`ã§é‡è¤‡å‰Šé™¤
- `mean()`ã§å¹³å‡å€¤è¨ˆç®—
- `to_csv()`ã§ä¿å­˜

</details>

---

## ğŸ“– æ¨¡ç¯„è§£ç­”

### è§£ç­”1: åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—

```python
"""
å•é¡Œ1ã®æ¨¡ç¯„è§£ç­”: åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv

load_dotenv()

# StatsDataReaderã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
reader = StatsDataReader()

# çµ±è¨ˆè¡¨IDã‚’æŒ‡å®š
stats_id = "0003410379"

print("ã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)
print(f"çµ±è¨ˆè¡¨ID: {stats_id}")

# ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
df = reader.read(statsDataId=stats_id, limit=1000)

# ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’è¡¨ç¤º
rows, cols = df.shape
print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
print(f"   å½¢çŠ¶: {rows}è¡Œ Ã— {cols}åˆ—")

# ã‚«ãƒ©ãƒ åã‚’è¡¨ç¤º
print(f"\nã€ã‚«ãƒ©ãƒ ä¸€è¦§ã€‘({len(df.columns)}å€‹)")
print("-" * 60)

for i, col in enumerate(df.columns, 1):
    print(f"{i:2}. {col}")

# ãƒ‡ãƒ¼ã‚¿å‹ã‚‚è¡¨ç¤º
print(f"\nã€ãƒ‡ãƒ¼ã‚¿å‹ã€‘")
print("-" * 60)
print(df.dtypes)

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
print(f"\nã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€‘")
print(f"ç´„ {memory_mb:.2f} MB")
```

---

### è§£ç­”2: åœ°åŸŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
"""
å•é¡Œ2ã®æ¨¡ç¯„è§£ç­”: ç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨åˆ†æ
"""
from jpy_datareader.estat import MetaInfoReader, StatsDataReader
from dotenv import load_dotenv

load_dotenv()

print("ã€ç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰ç¦å²¡çœŒã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
print("\nã‚¹ãƒ†ãƒƒãƒ—1: ç¦å²¡çœŒã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢")
print("-" * 60)

meta_reader = MetaInfoReader()
df_meta = meta_reader.read(statsDataId=stats_id)

# åœ°åŸŸåˆ†é¡ã‚’æŠ½å‡º
df_area = df_meta[df_meta['@class'] == 'area']

# ç¦å²¡çœŒã‚’æ¤œç´¢
df_fukuoka = df_area[df_area['@name'].str.contains('ç¦å²¡çœŒ', na=False)]

if len(df_fukuoka) == 0:
    print("âŒ ç¦å²¡çœŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
else:
    fukuoka_code = df_fukuoka.iloc[0]['@code']
    fukuoka_name = df_fukuoka.iloc[0]['@name']

    print(f"âœ… ç¦å²¡çœŒã‚³ãƒ¼ãƒ‰: [{fukuoka_code}] {fukuoka_name}")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("\nã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
    print("-" * 60)

    data_reader = StatsDataReader()
    df = data_reader.read(
        statsDataId=stats_id,
        cdArea=fukuoka_code,
        limit=500
    )

    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

    # ã‚¹ãƒ†ãƒƒãƒ—3: æ•°å€¤ã‚«ãƒ©ãƒ ã®åŸºæœ¬çµ±è¨ˆé‡
    print("\nã‚¹ãƒ†ãƒƒãƒ—3: åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—")
    print("-" * 60)

    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

    if len(numeric_cols) > 0:
        print(f"æ•°å€¤ã‚«ãƒ©ãƒ æ•°: {len(numeric_cols)}\n")

        # ã™ã¹ã¦ã®æ•°å€¤ã‚«ãƒ©ãƒ ã®åŸºæœ¬çµ±è¨ˆé‡
        print("ã€åŸºæœ¬çµ±è¨ˆé‡ã€‘")
        print(df[numeric_cols].describe())

        # å€‹åˆ¥ã«è©³ç´°ã‚’è¡¨ç¤º
        print("\nã€å„ã‚«ãƒ©ãƒ ã®è©³ç´°ã€‘")
        for col in numeric_cols:
            print(f"\nâ–  {col}")
            print(f"  å¹³å‡å€¤: {df[col].mean():.2f}")
            print(f"  ä¸­å¤®å€¤: {df[col].median():.2f}")
            print(f"  æ¨™æº–åå·®: {df[col].std():.2f}")
            print(f"  æœ€å°å€¤: {df[col].min():.2f}")
            print(f"  æœ€å¤§å€¤: {df[col].max():.2f}")
            print(f"  æ¬ æå€¤: {df[col].isnull().sum()}ä»¶")
    else:
        print("âš ï¸ æ•°å€¤ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
```

---

### è§£ç­”3: è¤‡æ•°æ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```python
"""
å•é¡Œ3ã®æ¨¡ç¯„è§£ç­”: åŒ—æµ·é“ã®2023å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—
"""
from jpy_datareader.estat import MetaInfoReader, StatsDataReader
from dotenv import load_dotenv
import os

load_dotenv()

print("ã€åŒ—æµ·é“ã®2023å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

# ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
meta_reader = MetaInfoReader()
df_meta = meta_reader.read(statsDataId=stats_id)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™
params = {
    'statsDataId': stats_id,
    'limit': 1000
}

# ã‚¹ãƒ†ãƒƒãƒ—1: åŒ—æµ·é“ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
print("\nã‚¹ãƒ†ãƒƒãƒ—1: åŒ—æµ·é“ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢")
print("-" * 60)

df_area = df_meta[df_meta['@class'] == 'area']
df_hokkaido = df_area[df_area['@name'].str.contains('åŒ—æµ·é“', na=False)]

if len(df_hokkaido) > 0:
    params['cdArea'] = df_hokkaido.iloc[0]['@code']
    print(f"âœ… åœ°åŸŸ: {df_hokkaido.iloc[0]['@name']} [{params['cdArea']}]")
else:
    print("âŒ åŒ—æµ·é“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# ã‚¹ãƒ†ãƒƒãƒ—2: 2023å¹´ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
print("\nã‚¹ãƒ†ãƒƒãƒ—2: 2023å¹´ã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢")
print("-" * 60)

df_time = df_meta[df_meta['@class'] == 'time']
df_2023 = df_time[df_time['@name'].str.contains('2023', na=False)]

if len(df_2023) > 0:
    params['cdTime'] = df_2023.iloc[0]['@code']
    print(f"âœ… æ™‚é–“: {df_2023.iloc[0]['@name']} [{params['cdTime']}]")

    # 2023å¹´ã®ãƒ‡ãƒ¼ã‚¿ãŒã„ãã¤ã‚ã‚‹ã‹è¡¨ç¤º
    print(f"   2023å¹´ã®æ™‚ç‚¹æ•°: {len(df_2023)}ä»¶")

    if len(df_2023) > 1:
        print("\n   2023å¹´ã®æ™‚ç‚¹ä¸€è¦§:")
        for idx, row in df_2023.head(5).iterrows():
            print(f"     - [{row['@code']}] {row['@name']}")
else:
    print("âš ï¸ 2023å¹´ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
print("\nã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
print("-" * 60)

if 'cdArea' in params and 'cdTime' in params:
    data_reader = StatsDataReader()
    df = data_reader.read(**params)

    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")

    # ã‚¹ãƒ†ãƒƒãƒ—4: CSVã«ä¿å­˜
    print("\nã‚¹ãƒ†ãƒƒãƒ—4: CSVã«ä¿å­˜")
    print("-" * 60)

    output_file = "hokkaido_2023_data.csv"
    df.to_csv(output_file, index=False, encoding='utf-8-sig')

    file_size = os.path.getsize(output_file)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes ({file_size/1024:.1f} KB)")

    # ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
    print("\nã€ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘")
    print(df.head(10))

else:
    print("\nâŒ å¿…è¦ãªæ¡ä»¶ãŒæƒã„ã¾ã›ã‚“ã§ã—ãŸ")
```

---

### è§£ç­”4: ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³

```python
"""
å•é¡Œ4ã®æ¨¡ç¯„è§£ç­”: æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
"""
from jpy_datareader.estat import StatsDataReader
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

print("ã€æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"
page_size = 10000
num_pages = 3

reader = StatsDataReader()
all_pages = []

# å„ãƒšãƒ¼ã‚¸ã‚’å–å¾—
for page_num in range(num_pages):
    start_pos = page_num * page_size + 1

    print(f"\nã€ãƒšãƒ¼ã‚¸ {page_num + 1}/{num_pages}ã€‘")
    print(f"é–‹å§‹ä½ç½®: {start_pos:,}")

    try:
        df_page = reader.read(
            statsDataId=stats_id,
            startPosition=start_pos,
            limit=page_size
        )

        print(f"âœ… {len(df_page):,}ä»¶å–å¾—")

        # ãƒšãƒ¼ã‚¸ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        all_pages.append(df_page)

        # ãƒ‡ãƒ¼ã‚¿ãŒpage_sizeã‚ˆã‚Šå°‘ãªã‘ã‚Œã°çµ‚äº†
        if len(df_page) < page_size:
            print(f"âš ï¸ æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”ï¼ˆ{len(df_page):,}ä»¶ã®ã¿å–å¾—ï¼‰")
            break

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        break

# ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’çµåˆ
print("\nã€ãƒ‡ãƒ¼ã‚¿ã®çµåˆã€‘")
print("=" * 60)

if all_pages:
    df_combined = pd.concat(all_pages, ignore_index=True)

    print(f"âœ… çµåˆå®Œäº†")
    print(f"   ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_combined):,}ä»¶")
    print(f"   ã‚«ãƒ©ãƒ æ•°: {len(df_combined.columns)}åˆ—")
    print(f"   ãƒšãƒ¼ã‚¸æ•°: {len(all_pages)}ãƒšãƒ¼ã‚¸")

    # å„ãƒšãƒ¼ã‚¸ã®ä»¶æ•°ã‚’è¡¨ç¤º
    print("\nã€ãƒšãƒ¼ã‚¸åˆ¥ä»¶æ•°ã€‘")
    for i, df_page in enumerate(all_pages, 1):
        print(f"   ãƒšãƒ¼ã‚¸{i}: {len(df_page):,}ä»¶")

    # ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­ã¨æœ«å°¾ã‚’è¡¨ç¤º
    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œã€‘")
    print(df_combined.head())

    print("\nã€ãƒ‡ãƒ¼ã‚¿ã®æœ«å°¾5è¡Œã€‘")
    print(df_combined.tail())

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = df_combined.duplicated().sum()
    print(f"\nã€é‡è¤‡ãƒã‚§ãƒƒã‚¯ã€‘")
    print(f"   é‡è¤‡è¡Œæ•°: {duplicates:,}ä»¶")

    # ä¿å­˜
    output_file = "paginated_data.csv"
    df_combined.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")

else:
    print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
```

---

### è§£ç­”5: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

```python
"""
å•é¡Œ5ã®æ¨¡ç¯„è§£ç­”: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
"""
from jpy_datareader.estat import StatsDataReader
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

print("ã€ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘")
print("=" * 60)

stats_id = "0003410379"

# ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
reader = StatsDataReader()
df_raw = reader.read(statsDataId=stats_id, limit=5000)

print(f"\nã€å…ƒãƒ‡ãƒ¼ã‚¿ã€‘")
print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_raw):,}ä»¶")
print(f"ã‚«ãƒ©ãƒ æ•°: {len(df_raw.columns)}åˆ—")

# ===== ã‚¹ãƒ†ãƒƒãƒ—1: æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ =====
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘æ¬ æå€¤ã®å‡¦ç†")
print("-" * 60)

# æ¬ æå€¤ã®ç¢ºèª
missing_before = df_raw.isnull().sum().sum()
print(f"æ¬ æå€¤ã®ç·æ•°: {missing_before:,}å€‹")

# æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
df_clean = df_raw.dropna()

removed_by_na = len(df_raw) - len(df_clean)
print(f"âœ… æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤: {removed_by_na:,}è¡Œ")
print(f"   æ®‹ã‚Š: {len(df_clean):,}è¡Œ")

# ===== ã‚¹ãƒ†ãƒƒãƒ—2: é‡è¤‡è¡Œã‚’å‰Šé™¤ =====
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘é‡è¤‡è¡Œã®å‡¦ç†")
print("-" * 60)

duplicates_count = df_clean.duplicated().sum()
print(f"é‡è¤‡è¡Œæ•°: {duplicates_count:,}è¡Œ")

df_clean = df_clean.drop_duplicates()

removed_by_dup = duplicates_count
print(f"âœ… é‡è¤‡è¡Œã‚’å‰Šé™¤: {removed_by_dup:,}è¡Œ")
print(f"   æ®‹ã‚Š: {len(df_clean):,}è¡Œ")

# ===== ã‚¹ãƒ†ãƒƒãƒ—3: æ•°å€¤ã‚«ãƒ©ãƒ ã®å¹³å‡å€¤ã‚’è¨ˆç®— =====
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘æ•°å€¤ã‚«ãƒ©ãƒ ã®å¹³å‡å€¤")
print("-" * 60)

numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns

if len(numeric_cols) > 0:
    print(f"æ•°å€¤ã‚«ãƒ©ãƒ æ•°: {len(numeric_cols)}\n")

    # å„ã‚«ãƒ©ãƒ ã®å¹³å‡å€¤ã‚’è¨ˆç®—
    for col in numeric_cols:
        mean_value = df_clean[col].mean()
        median_value = df_clean[col].median()

        print(f"â–  {col}")
        print(f"  å¹³å‡å€¤: {mean_value:,.2f}")
        print(f"  ä¸­å¤®å€¤: {median_value:,.2f}")
        print()

    # å¹³å‡å€¤ã®ã‚µãƒãƒªãƒ¼DataFrameã‚’ä½œæˆ
    summary = pd.DataFrame({
        'ã‚«ãƒ©ãƒ ': numeric_cols,
        'å¹³å‡å€¤': [df_clean[col].mean() for col in numeric_cols],
        'ä¸­å¤®å€¤': [df_clean[col].median() for col in numeric_cols],
        'æ¨™æº–åå·®': [df_clean[col].std() for col in numeric_cols],
        'æœ€å°å€¤': [df_clean[col].min() for col in numeric_cols],
        'æœ€å¤§å€¤': [df_clean[col].max() for col in numeric_cols]
    })

    print("ã€çµ±è¨ˆã‚µãƒãƒªãƒ¼ã€‘")
    print(summary.to_string(index=False))

else:
    print("âš ï¸ æ•°å€¤ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# ===== ã‚¹ãƒ†ãƒƒãƒ—4: å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜ =====
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜")
print("-" * 60)

output_file = "cleaned_data.csv"
df_clean.to_csv(output_file, index=False, encoding='utf-8-sig')

print(f"âœ… ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜")
print(f"   æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_clean):,}è¡Œ")
print(f"   å‰Šé™¤ã•ã‚ŒãŸè¡Œæ•°: {len(df_raw) - len(df_clean):,}è¡Œ")
print(f"   ä¿æŒç‡: {(len(df_clean) / len(df_raw) * 100):.1f}%")

# ã‚µãƒãƒªãƒ¼ã‚‚ä¿å­˜
if len(numeric_cols) > 0:
    summary_file = "data_summary.csv"
    summary.to_csv(summary_file, index=False, encoding='utf-8-sig')
    print(f"âœ… çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ {summary_file} ã«ä¿å­˜")

# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœã®ãƒ¬ãƒãƒ¼ãƒˆ
print("\nã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆã€‘")
print("=" * 60)
print(f"å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df_raw):,}è¡Œ")
print(f"æ¬ æå€¤å‰Šé™¤: -{removed_by_na:,}è¡Œ")
print(f"é‡è¤‡å‰Šé™¤: -{removed_by_dup:,}è¡Œ")
print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(df_clean):,}è¡Œ")
print(f"å‰Šé™¤ç‡: {((len(df_raw) - len(df_clean)) / len(df_raw) * 100):.1f}%")
```

---

## ğŸ¯ ã¾ã¨ã‚

ã“ã®ç« ã§ã¯ä»¥ä¸‹ã‚’å­¦ã³ã¾ã—ãŸï¼š

âœ… StatsDataReaderã‚¯ãƒ©ã‚¹ã®è©³ç´°ãªä½¿ã„æ–¹
âœ… æ§˜ã€…ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆåœ°åŸŸã€æ™‚é–“ã€ã‚«ãƒ†ã‚´ãƒªï¼‰
âœ… å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
âœ… æ‰‹å‹•ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ¶å¾¡æ–¹æ³•
âœ… ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“
âœ… åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—æˆ¦ç•¥

---

## ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æ¬¡ã®ç« ã§ã¯ã€å®Ÿè·µçš„ãªãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å­¦ã³ã¾ã™ï¼š

**[ç¬¬5ç« : ä¸Šç´šç·¨ - ãƒ‡ãƒ¼ã‚¿åˆ†æã®å®Ÿè·µ](./chapter05_advanced_analysis.md)**

- è¤‡æ•°çµ±è¨ˆè¡¨ã®çµåˆ
- æ™‚ç³»åˆ—åˆ†æ
- åœ°åŸŸæ¯”è¼ƒåˆ†æ
- ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–
- å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

---

ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼

# ç¬¬6ç« : ä¸Šç´šç·¨ - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

## ğŸ“š ã“ã®ç« ã§å­¦ã¶ã“ã¨

- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®è©³ç´°
- ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨

**å¯¾è±¡ãƒ¬ãƒ™ãƒ«**: ä¸Šç´šè€…
**æ‰€è¦æ™‚é–“**: ç´„60åˆ†
**å‰æçŸ¥è­˜**: ç¬¬1ã€œ5ç« ã®å†…å®¹ã€Pythonã®ä¾‹å¤–å‡¦ç†

---

## 6.1 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### 6.1.1 åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†

```python
"""
åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv
import time

load_dotenv()

def safe_data_fetch(stats_id, **kwargs):
    """
    å®‰å…¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    **kwargs : dict
        ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
    --------
    tuple : (success, data_or_error)
        success: bool - æˆåŠŸã—ãŸã‹ã©ã†ã‹
        data_or_error: DataFrame or Exception - ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼
    """
    reader = StatsDataReader()

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = reader.read(statsDataId=stats_id, **kwargs)

        # æˆåŠŸ
        return True, df

    except ValueError as e:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼
        print(f"âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
        return False, e

    except ConnectionError as e:
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼
        print(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return False, e

    except TimeoutError as e:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        print(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
        return False, e

    except Exception as e:
        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        return False, e


# ä½¿ç”¨ä¾‹
print("ã€å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

success, result = safe_data_fetch(stats_id, limit=100)

if success:
    print(f"âœ… æˆåŠŸ: {len(result)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
else:
    print(f"âŒ å¤±æ•—: {result}")
```

### 6.1.2 ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…

```python
"""
ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãã®ãƒ‡ãƒ¼ã‚¿å–å¾—
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv
import time

load_dotenv()

def fetch_with_retry(
    stats_id,
    max_retries=3,
    retry_delay=2,
    backoff_factor=2,
    **kwargs
):
    """
    ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®ãƒ‡ãƒ¼ã‚¿å–å¾—

    Parameters:
    -----------
    stats_id : str
        çµ±è¨ˆè¡¨ID
    max_retries : int
        æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
    retry_delay : float
        åˆæœŸãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    backoff_factor : float
        å¾…æ©Ÿæ™‚é–“ã®å¢—åŠ ç‡
    **kwargs : dict
        ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

    Returns:
    --------
    pandas.DataFrame or None
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã€å¤±æ•—æ™‚ã¯None
    """
    reader = StatsDataReader()

    for attempt in range(max_retries + 1):
        try:
            print(f"\nè©¦è¡Œ {attempt + 1}/{max_retries + 1}...")

            # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            df = reader.read(statsDataId=stats_id, **kwargs)

            # æˆåŠŸ
            print(f"âœ… æˆåŠŸ: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
            return df

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

            # æœ€å¾Œã®è©¦è¡Œãªã‚‰è«¦ã‚ã‚‹
            if attempt == max_retries:
                print(f"\nâŒ {max_retries + 1}å›è©¦è¡Œã—ã¾ã—ãŸãŒå¤±æ•—ã—ã¾ã—ãŸ")
                return None

            # å¾…æ©Ÿæ™‚é–“ã‚’è¨ˆç®—ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
            wait_time = retry_delay * (backoff_factor ** attempt)
            print(f"â³ {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")

            time.sleep(wait_time)

    return None


# ä½¿ç”¨ä¾‹
print("ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

stats_id = "0003410379"

df = fetch_with_retry(
    stats_id,
    limit=100,
    max_retries=3,
    retry_delay=1,
    backoff_factor=2
)

if df is not None:
    print(f"\næœ€çµ‚çµæœ: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
else:
    print("\næœ€çµ‚çµæœ: ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
```

---

## 6.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 6.2.1 ãƒãƒƒãƒå‡¦ç†

```python
"""
åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv
import pandas as pd
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

load_dotenv()

class BatchDataFetcher:
    """
    ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, max_workers=3):
        """
        åˆæœŸåŒ–

        Parameters:
        -----------
        max_workers : int
            ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.max_workers = max_workers
        self.reader = StatsDataReader()

    def fetch_single(self, stats_id, **kwargs):
        """
        å˜ä¸€ã®çµ±è¨ˆè¡¨ã‚’å–å¾—

        Parameters:
        -----------
        stats_id : str
            çµ±è¨ˆè¡¨ID
        **kwargs : dict
            ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
        --------
        tuple : (stats_id, DataFrame or None)
        """
        try:
            print(f"  [{stats_id}] å–å¾—é–‹å§‹...")
            start_time = time.time()

            df = self.reader.read(statsDataId=stats_id, **kwargs)

            elapsed = time.time() - start_time
            print(f"  [{stats_id}] å®Œäº† ({elapsed:.2f}ç§’, {len(df)}ä»¶)")

            return stats_id, df

        except Exception as e:
            print(f"  [{stats_id}] ã‚¨ãƒ©ãƒ¼: {e}")
            return stats_id, None

    def fetch_batch(self, stats_ids, **kwargs):
        """
        è¤‡æ•°ã®çµ±è¨ˆè¡¨ã‚’ä¸¦åˆ—ã§å–å¾—

        Parameters:
        -----------
        stats_ids : list
            çµ±è¨ˆè¡¨IDã®ãƒªã‚¹ãƒˆ
        **kwargs : dict
            å„çµ±è¨ˆè¡¨ã«å…±é€šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
        --------
        dict : {stats_id: DataFrame}
        """
        print(f"ã€ãƒãƒƒãƒå–å¾—é–‹å§‹ã€‘")
        print(f"çµ±è¨ˆè¡¨æ•°: {len(stats_ids)}")
        print(f"ä¸¦åˆ—æ•°: {self.max_workers}")
        print("=" * 60)

        results = {}
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
            futures = {
                executor.submit(self.fetch_single, sid, **kwargs): sid
                for sid in stats_ids
            }

            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰é †ã«å‡¦ç†
            for future in as_completed(futures):
                stats_id, df = future.result()
                if df is not None:
                    results[stats_id] = df

        elapsed = time.time() - start_time

        print("\n" + "=" * 60)
        print(f"ã€ãƒãƒƒãƒå–å¾—å®Œäº†ã€‘")
        print(f"æˆåŠŸ: {len(results)}/{len(stats_ids)}ä»¶")
        print(f"ç·æ‰€è¦æ™‚é–“: {elapsed:.2f}ç§’")
        print(f"å¹³å‡: {elapsed/len(stats_ids):.2f}ç§’/çµ±è¨ˆè¡¨")

        return results


# ä½¿ç”¨ä¾‹
print("ã€ãƒãƒƒãƒå‡¦ç†ã®ä¾‹ã€‘")
print("=" * 60)

# è¤‡æ•°ã®çµ±è¨ˆè¡¨IDã‚’æº–å‚™
stats_ids = [
    "0003410379",
    "0003348423",
    # ä»–ã®çµ±è¨ˆè¡¨IDã‚’è¿½åŠ 
]

fetcher = BatchDataFetcher(max_workers=3)
results = fetcher.fetch_batch(stats_ids, limit=100)

# çµæœã‚’ç¢ºèª
for stats_id, df in results.items():
    print(f"\nçµ±è¨ˆè¡¨ {stats_id}: {len(df)}ä»¶")
```

---

## 6.3 ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

### 6.3.1 ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…

```python
"""
ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…
"""
from jpy_datareader.estat import StatsDataReader
from dotenv import load_dotenv
import pandas as pd
import pickle
import os
from datetime import datetime, timedelta
import hashlib

load_dotenv()

class CachedDataReader:
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ãƒ€ãƒ¼
    """

    def __init__(self, cache_dir="./cache", cache_expiry_hours=24):
        """
        åˆæœŸåŒ–

        Parameters:
        -----------
        cache_dir : str
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cache_expiry_hours : int
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹æœŸé™ï¼ˆæ™‚é–“ï¼‰
        """
        self.cache_dir = cache_dir
        self.cache_expiry = timedelta(hours=cache_expiry_hours)
        self.reader = StatsDataReader()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, stats_id, **kwargs):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ

        Parameters:
        -----------
        stats_id : str
            çµ±è¨ˆè¡¨ID
        **kwargs : dict
            ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
        --------
        str : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
        """
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        params_str = f"{stats_id}_{str(sorted(kwargs.items()))}"

        # ãƒãƒƒã‚·ãƒ¥åŒ–
        key = hashlib.md5(params_str.encode()).hexdigest()

        return key

    def _get_cache_path(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")

    def _is_cache_valid(self, cache_path):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯

        Parameters:
        -----------
        cache_path : str
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
        --------
        bool : æœ‰åŠ¹ãªã‚‰True
        """
        if not os.path.exists(cache_path):
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
        mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))

        # æœ‰åŠ¹æœŸé™ã‚’ãƒã‚§ãƒƒã‚¯
        return (datetime.now() - mtime) < self.cache_expiry

    def read(self, stats_id, use_cache=True, **kwargs):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼‰

        Parameters:
        -----------
        stats_id : str
            çµ±è¨ˆè¡¨ID
        use_cache : bool
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        **kwargs : dict
            ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
        --------
        pandas.DataFrame
        """
        cache_key = self._get_cache_key(stats_id, **kwargs)
        cache_path = self._get_cache_path(cache_key)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        if use_cache and self._is_cache_valid(cache_path):
            print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿: {cache_key}")

            with open(cache_path, 'rb') as f:
                df = pickle.load(f)

            return df

        # APIã‹ã‚‰å–å¾—
        print(f"ğŸŒ APIã‹ã‚‰å–å¾—: {stats_id}")
        df = self.reader.read(statsDataId=stats_id, **kwargs)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if use_cache:
            with open(cache_path, 'wb') as f:
                pickle.dump(df, f)

            print(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {cache_key}")

        return df

    def clear_cache(self, older_than_hours=None):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢

        Parameters:
        -----------
        older_than_hours : int, optional
            æŒ‡å®šæ™‚é–“ã‚ˆã‚Šå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å‰Šé™¤
        """
        print(f"ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã€‘")
        print("=" * 60)

        cleared = 0

        for filename in os.listdir(self.cache_dir):
            if not filename.endswith('.pkl'):
                continue

            filepath = os.path.join(self.cache_dir, filename)

            # å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿å‰Šé™¤ã™ã‚‹å ´åˆ
            if older_than_hours is not None:
                mtime = datetime.fromtimestamp(os.path.getmtime(filepath))
                age = datetime.now() - mtime

                if age < timedelta(hours=older_than_hours):
                    continue

            # å‰Šé™¤
            os.remove(filepath)
            cleared += 1

        print(f"âœ… {cleared}ä»¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")


# ä½¿ç”¨ä¾‹
print("ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—ã€‘")
print("=" * 60)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒªãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
cached_reader = CachedDataReader(
    cache_dir="./data_cache",
    cache_expiry_hours=24
)

stats_id = "0003410379"

# 1å›ç›®: APIã‹ã‚‰å–å¾—
print("\n1å›ç›®ã®å–å¾—:")
df1 = cached_reader.read(stats_id, limit=100)
print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df1)}")

# 2å›ç›®: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
print("\n2å›ç›®ã®å–å¾—:")
df2 = cached_reader.read(stats_id, limit=100)
print(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df2)}")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
print("\n")
cached_reader.clear_cache(older_than_hours=0)  # ã™ã¹ã¦ã‚¯ãƒªã‚¢
```

---

## 6.4 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 6.4.1 APIã‚­ãƒ¼ã®å®‰å…¨ãªç®¡ç†

```python
"""
APIã‚­ãƒ¼ã®å®‰å…¨ãªç®¡ç†
"""
import os
from dotenv import load_dotenv
from pathlib import Path

# âŒ æ‚ªã„ä¾‹: ã‚³ãƒ¼ãƒ‰ã«ç›´æ¥è¨˜è¿°
# BAD_API_KEY = "your_api_key_here"  # çµ¶å¯¾ã«ã‚„ã‚‰ãªã„ï¼

# âœ… è‰¯ã„ä¾‹: ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨
load_dotenv()
api_key = os.getenv('ESTAT_APP_ID')

if not api_key:
    raise ValueError("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

print("âœ… APIã‚­ãƒ¼ãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ")


# .envãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
def validate_env_file():
    """
    .envãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼
    """
    print("ã€ç’°å¢ƒå¤‰æ•°ã®æ¤œè¨¼ã€‘")
    print("=" * 60)

    # .envãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    env_path = Path('.env')

    if not env_path.exists():
        print("âŒ .envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("\n.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„:")
        print("  ESTAT_APP_ID=your_app_id_here")
        return False

    print("âœ… .envãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã™")

    # .gitignoreã®ç¢ºèª
    gitignore_path = Path('.gitignore')

    if gitignore_path.exists():
        with open(gitignore_path, 'r') as f:
            content = f.read()

        if '.env' in content:
            print("âœ… .gitignoreã«.envãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        else:
            print("âš ï¸ .gitignoreã«.envã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼")
    else:
        print("âš ï¸ .gitignoreãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’Gitã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã‚ˆã†æ³¨æ„ã—ã¦ãã ã•ã„")

    # APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ç¢ºèª
    load_dotenv()
    api_key = os.getenv('ESTAT_APP_ID')

    if api_key:
        # APIã‚­ãƒ¼ã®ä¸€éƒ¨ã ã‘è¡¨ç¤º
        masked_key = api_key[:5] + "..." + api_key[-3:] if len(api_key) > 8 else "***"
        print(f"âœ… APIã‚­ãƒ¼ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ: {masked_key}")
        return True
    else:
        print("âŒ APIã‚­ãƒ¼ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“")
        return False


# æ¤œè¨¼ã‚’å®Ÿè¡Œ
validate_env_file()
```

### 6.4.2 ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªä¿å­˜

```python
"""
ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªä¿å­˜
"""
import pandas as pd
import os
from pathlib import Path

class SecureDataSaver:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, output_dir="./output"):
        """
        åˆæœŸåŒ–

        Parameters:
        -----------
        output_dir : str
            å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.output_dir = Path(output_dir)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®šï¼ˆUnixã‚·ã‚¹ãƒ†ãƒ ã®ã¿ï¼‰
        if os.name != 'nt':  # Windowsä»¥å¤–
            os.chmod(self.output_dir, 0o700)  # æ‰€æœ‰è€…ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯

    def save_csv(self, df, filename, include_sensitive=False):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜

        Parameters:
        -----------
        df : pandas.DataFrame
            ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        filename : str
            ãƒ•ã‚¡ã‚¤ãƒ«å
        include_sensitive : bool
            æ©Ÿå¯†æƒ…å ±ã‚’å«ã‚€ã‹ã©ã†ã‹
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        filepath = self.output_dir / filename

        # æ©Ÿå¯†æƒ…å ±ã‚’å«ã‚€å ´åˆã¯è­¦å‘Š
        if include_sensitive:
            print("âš ï¸ æ©Ÿå¯†æƒ…å ±ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {filepath}")
            print("   é©åˆ‡ã«ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚’è¨­å®šã—ã¦ãã ã•ã„")

        # ä¿å­˜
        df.to_csv(filepath, index=False, encoding='utf-8-sig')

        # ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®šï¼ˆUnixã‚·ã‚¹ãƒ†ãƒ ã®ã¿ï¼‰
        if os.name != 'nt':  # Windowsä»¥å¤–
            os.chmod(filepath, 0o600)  # æ‰€æœ‰è€…ã®ã¿èª­ã¿æ›¸ãå¯

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")

        return filepath

    def save_encrypted(self, df, filename, password):
        """
        æš—å·åŒ–ã—ã¦ä¿å­˜ï¼ˆç°¡æ˜“ç‰ˆï¼‰

        Parameters:
        -----------
        df : pandas.DataFrame
            ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
        filename : str
            ãƒ•ã‚¡ã‚¤ãƒ«å
        password : str
            ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰

        Note:
        -----
        å®Ÿéš›ã®æœ¬ç•ªç’°å¢ƒã§ã¯ã€ã‚ˆã‚Šå¼·åŠ›ãªæš—å·åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
        ï¼ˆcryptographyç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
        """
        print("âš ï¸ ã“ã®å®Ÿè£…ã¯æ•™è‚²ç›®çš„ã§ã™")
        print("   æœ¬ç•ªç’°å¢ƒã§ã¯ã‚ˆã‚Šå¼·åŠ›ãªæš—å·åŒ–ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

        # ã“ã“ã§ã¯æš—å·åŒ–ã®è©³ç´°ã¯çœç•¥
        # å®Ÿéš›ã«ã¯ cryptography ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã©ã‚’ä½¿ç”¨

        filepath = self.output_dir / filename
        df.to_csv(filepath, index=False, encoding='utf-8-sig')

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")


# ä½¿ç”¨ä¾‹
saver = SecureDataSaver(output_dir="./secure_output")

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
df = pd.DataFrame({
    'id': [1, 2, 3],
    'value': [100, 200, 300]
})

# ä¿å­˜
saver.save_csv(df, "data.csv", include_sensitive=False)
```

---

## 6.5 æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨

### 6.5.1 ãƒ­ã‚®ãƒ³ã‚°ã®å®Ÿè£…

```python
"""
ãƒ­ã‚®ãƒ³ã‚°ã®å®Ÿè£…
"""
import logging
from datetime import datetime
import os
from pathlib import Path

class DataFetchLogger:
    """
    ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãƒ­ã‚®ãƒ³ã‚°ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, log_dir="./logs"):
        """
        åˆæœŸåŒ–

        Parameters:
        -----------
        log_dir : str
            ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ­ã‚¬ãƒ¼ã‚’è¨­å®š
        self.logger = logging.getLogger('JPyDataReader')
        self.logger.setLevel(logging.DEBUG)

        # ã™ã§ã«ãƒãƒ³ãƒ‰ãƒ©ãŒè¨­å®šã•ã‚Œã¦ã„ãŸã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if not self.logger.handlers:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
            log_file = self.log_dir / f"data_fetch_{datetime.now():%Y%m%d}.log"
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)

            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)

            # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)

            # ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
            self.logger.addHandler(file_handler)
            self.logger.addHandler(console_handler)

    def log_fetch_start(self, stats_id, params):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ã‚’ãƒ­ã‚°"""
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: stats_id={stats_id}, params={params}")

    def log_fetch_success(self, stats_id, row_count, elapsed_time):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸã‚’ãƒ­ã‚°"""
        self.logger.info(
            f"ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: stats_id={stats_id}, "
            f"rows={row_count}, elapsed={elapsed_time:.2f}s"
        )

    def log_fetch_error(self, stats_id, error):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°"""
        self.logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: stats_id={stats_id}, error={error}")

    def log_cache_hit(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’ãƒ­ã‚°"""
        self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: key={cache_key}")

    def log_cache_miss(self, cache_key):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã‚’ãƒ­ã‚°"""
        self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: key={cache_key}")


# ä½¿ç”¨ä¾‹
logger = DataFetchLogger()

logger.log_fetch_start("0003410379", {"limit": 100})
logger.log_fetch_success("0003410379", 100, 1.5)
logger.log_cache_hit("abc123")
```

---

## ğŸ“ ç·´ç¿’å•é¡Œ

### å•é¡Œ1: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**èª²é¡Œ**: ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§ã€3å›å¤±æ•—ã—ãŸã‚‰ãƒ¡ãƒ¼ãƒ«ã§é€šçŸ¥ã™ã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

### å•é¡Œ2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

**èª²é¡Œ**: ãƒ‡ãƒ¼ã‚¿å–å¾—ã®æ‰€è¦æ™‚é–“ã‚’æ¸¬å®šã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### å•é¡Œ3: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–

**èª²é¡Œ**: LRUï¼ˆLeast Recently Usedï¼‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

### å•é¡Œ4: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯

**èª²é¡Œ**: ãƒ‡ãƒ¼ã‚¿ã«å€‹äººæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### å•é¡Œ5: æœ¬ç•ªé‹ç”¨ã‚·ã‚¹ãƒ†ãƒ 

**èª²é¡Œ**: ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤æœ¬ç•ªé‹ç”¨å¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ï¼š
1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
2. ãƒ­ã‚®ãƒ³ã‚°
3. ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

---

## ğŸ¯ ã¾ã¨ã‚

ã“ã®ç« ã§ã¯ä»¥ä¸‹ã‚’å­¦ã³ã¾ã—ãŸï¼š

âœ… åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
âœ… ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
âœ… ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
âœ… ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
âœ… æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
âœ… ãƒ­ã‚®ãƒ³ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

---

## ğŸ“ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œäº†ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼

å…¨6ç« ã‚’é€šã—ã¦ã€JPy-DataReaderã®åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§å­¦ç¿’ã—ã¾ã—ãŸã€‚

### å­¦ç¿’ã—ãŸå†…å®¹ã®å¾©ç¿’

**ç¬¬1ç« **: åŸºç¤ç·¨
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ç’°å¢ƒè¨­å®š
- åŸºæœ¬çš„ãª3ã¤ã®é–¢æ•°ã®ä½¿ã„æ–¹

**ç¬¬2ç« **: StatsListReader
- çµ±è¨ˆè¡¨ã®é«˜åº¦ãªæ¤œç´¢
- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆ

**ç¬¬3ç« **: MetaInfoReader
- ãƒ¡ã‚¿æƒ…å ±ã®ç†è§£ã¨æ´»ç”¨
- éšå±¤æ§‹é€ ã®åˆ†æ

**ç¬¬4ç« **: StatsDataReader
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå–å¾—
- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½

**ç¬¬5ç« **: ãƒ‡ãƒ¼ã‚¿åˆ†æã®å®Ÿè·µ
- æ™‚ç³»åˆ—åˆ†æ
- åœ°åŸŸæ¯”è¼ƒåˆ†æ
- å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

**ç¬¬6ç« **: ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å®Ÿè·µ**
   - è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å¿œç”¨
   - å­¦ã‚“ã ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’çµ„ã¿åˆã‚ã›ã‚‹

2. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®å‚åŠ **
   - GitHubã§ã®è³ªå•ãƒ»è­°è«–
   - æ”¹å–„ææ¡ˆã‚„ãƒã‚°å ±å‘Š

3. **ã•ã‚‰ãªã‚‹å­¦ç¿’**
   - pandas, matplotlib ã®é«˜åº¦ãªä½¿ã„æ–¹
   - çµ±è¨ˆåˆ†ææ‰‹æ³•
   - æ©Ÿæ¢°å­¦ç¿’ã¸ã®å¿œç”¨

---

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- [JPy-DataReader GitHub](https://github.com/kkawailab/jpy-datareader)
- [e-Stat API ä»•æ§˜](https://www.e-stat.go.jp/api/api-info/api-spec)
- [pandas Documentation](https://pandas.pydata.org/docs/)
- [Pythonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.python.org/ja/3/)

---

ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼
